{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, I will analyze text that was scrapped from twitter that I did in: [Twitter Scrapping with Python](https://github.com/teguhsam/twitter_scraping_with_PYTHON)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:37:56.755933Z",
     "start_time": "2020-02-18T07:37:56.747355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "with open('tweets_1000.json') as json_file:\n",
    "    json_text = json.load(json_file)\n",
    "\n",
    "print(type(json_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:37:56.790406Z",
     "start_time": "2020-02-18T07:37:56.762926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @SBSNews: The Morrison government is set to adopt a technology investment target instead of signing up to a global agreement to achieveâ€¦',\n",
       " '@ColinCowherd im for steroids and using technology to steal signs',\n",
       " \"@peachpanini of course we'd have the technology to show them on a reasonable scale via your tv/computer monitor. think of the possibilities\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_text[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text\n",
    "\n",
    "Resource: https://machinelearningmastery.com/clean-text-machine-learning-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:45:42.201511Z",
     "start_time": "2020-02-18T07:45:42.191059Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T05:05:26.974403Z",
     "start_time": "2020-02-18T05:05:26.970882Z"
    }
   },
   "source": [
    "## Create an empty list for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:09:09.882323Z",
     "start_time": "2020-02-18T08:09:09.877637Z"
    }
   },
   "outputs": [],
   "source": [
    "token_words_list = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:49:47.564911Z",
     "start_time": "2020-02-18T07:49:47.560298Z"
    }
   },
   "source": [
    "## Tokenize into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:09:11.921278Z",
     "start_time": "2020-02-18T08:09:11.734391Z"
    }
   },
   "outputs": [],
   "source": [
    "for index, tweet in enumerate(json_text):\n",
    "    #print(json_text[index])\n",
    "    tokenized_words = word_tokenize(json_text[index])\n",
    "    token_words_list.extend(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:09:46.327854Z",
     "start_time": "2020-02-18T08:09:46.320539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23997"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean words\n",
    "\n",
    "To analyze the text above, cleaning needs to be done to remove punctuations such as @,#,! etc. Stopwords such as: we, you, and, they are common words that are also need to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:18:00.615029Z",
     "start_time": "2020-02-18T08:18:00.609989Z"
    }
   },
   "outputs": [],
   "source": [
    "token_punc_removed = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:18:01.658999Z",
     "start_time": "2020-02-18T08:18:01.649113Z"
    }
   },
   "outputs": [],
   "source": [
    "for word in token_words_list:\n",
    "    if word.isalpha():\n",
    "        # before appending the word, put the word in a lower case\n",
    "        token_punc_removed.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:18:02.694687Z",
     "start_time": "2020-02-18T08:18:02.685991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16843"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_punc_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:17:38.269329Z",
     "start_time": "2020-02-18T08:17:38.080555Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/MBAN/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:21:15.035617Z",
     "start_time": "2020-02-18T08:21:15.030520Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:21:16.047107Z",
     "start_time": "2020-02-18T08:21:16.038073Z"
    }
   },
   "outputs": [],
   "source": [
    "token_stopwords_removed = list()\n",
    "\n",
    "for word in token_punc_removed:\n",
    "    if word not in stop_words:\n",
    "        token_stopwords_removed.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:22:19.029352Z",
     "start_time": "2020-02-18T08:22:19.024394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stopwords were removed, there are : 10941 remaining\n"
     ]
    }
   ],
   "source": [
    "print(\"After Stopwords were removed, there are : {} remaining\".format(len(token_stopwords_removed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:24:26.394386Z",
     "start_time": "2020-02-18T08:24:26.390011Z"
    }
   },
   "source": [
    "## Count word occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:24:42.656628Z",
     "start_time": "2020-02-18T08:24:42.651717Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:26:35.690680Z",
     "start_time": "2020-02-18T08:26:35.681162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.Counter"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = Counter(token_stopwords_removed)\n",
    "type(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:38:20.775641Z",
     "start_time": "2020-02-18T08:38:20.766899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>rt</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sbsnews</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>morrison</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>government</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>set</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "rt            626\n",
       "sbsnews         2\n",
       "morrison        3\n",
       "government     10\n",
       "set             4"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "word_counts_df = pd.DataFrame.from_dict(word_counts, \n",
    "                                        orient='index', \n",
    "                                        columns=['count'])\n",
    "word_counts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find 10 most popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:40:39.277960Z",
     "start_time": "2020-02-18T08:40:39.267909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>rt</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>https</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>technology</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>put</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>farmer</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>intelligence</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bloomberg</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anyone</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>farmers</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>data</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "rt              626\n",
       "https           456\n",
       "technology      276\n",
       "put              90\n",
       "farmer           83\n",
       "intelligence     80\n",
       "bloomberg        77\n",
       "anyone           74\n",
       "farmers          71\n",
       "data             64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_df.sort_values(by='count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see above that 'farmer' and 'farmers' are considered two different words. Lets Lemmatize!\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:50:50.211044Z",
     "start_time": "2020-02-18T08:50:50.179782Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/MBAN/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:50:51.915764Z",
     "start_time": "2020-02-18T08:50:51.907538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'farmer'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('farmers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:59:33.430400Z",
     "start_time": "2020-02-18T08:59:33.378630Z"
    }
   },
   "outputs": [],
   "source": [
    "word_lemmatized_list = list()\n",
    "for word in token_stopwords_removed:\n",
    "    w = wnl.lemmatize(word)\n",
    "    word_lemmatized_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:59:57.648468Z",
     "start_time": "2020-02-18T08:59:57.641203Z"
    }
   },
   "outputs": [],
   "source": [
    "word_counts = Counter(word_lemmatized_list)\n",
    "\n",
    "word_counts_df = pd.DataFrame.from_dict(word_counts, \n",
    "                                        orient='index', \n",
    "                                        columns=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T08:59:59.304556Z",
     "start_time": "2020-02-18T08:59:59.294417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>rt</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>technology</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>farmer</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>put</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>intelligence</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bloomberg</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anyone</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>teach</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>data</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "rt              626\n",
       "http            456\n",
       "technology      282\n",
       "farmer          154\n",
       "put              90\n",
       "intelligence     80\n",
       "bloomberg        77\n",
       "anyone           74\n",
       "teach            66\n",
       "data             64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_df.sort_values(by='count', ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
